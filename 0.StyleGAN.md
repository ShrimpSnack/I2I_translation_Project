# Style GAN

### 1. Progressive Growing
- progressive growing? : GAN에서 제안된 고해상도 이미지를 생성하는 방법
- 저해상도 이미지의 생성으로부터 시작해 점차 고해상도용의 Generator, Discriminator를 추가함
- 아래그림은 PG-GAN의 모식도이다.\
![image](https://user-images.githubusercontent.com/70633080/113817847-ef9d4880-97b1-11eb-8863-a43ca841c729.png)
- 4 * 4 의 이미지를 생성한 것으로 부터 시작해서 조금씩 해상도를 높여 최종적으로 1024 * 1024의 고해상도의 이미지를 생성해낸다.
- 해상도를 높이는 네트워크를 추가하여도 저해상도 이미지를 생성하는 G와 D는 파라미터가 고정되지않고 학습을 계속하는 것이 특징이다.
- 레이어(화질)이 낮을수록 내포된 특징이 다르다.
  - 1.Coarse(굵직한 특징) : 8해상도까지 (4 * 4 ~ 8 * 8) -> 포즈, 헤어스타일, 얼굴형에 영향
  - 2. Middle(중간 특징) : 16부터 32해상도 까지 -> 자세한 얼굴특징, 헤어스타일, 눈뜨고감음에 영향
  - 3. Fine(자세한 특징) : 64부터 1024까지 -> 눈,머리,피부등의 색조합과 미세한 특징에 영향 
### 2. Style-based generator
- 이는 2018년 12월의 Nvidia에서 올린 논문의 내용으로 GAN의 한계점을 극복할 수 있는 방향을 제시하였다.
- 어떤 GAN구조에도 바로적용할 수 있는 generator를 제안.
  - discriminator를 수정하거나 손실함수를 바꾸는 등 어떤 변경점도 없는 오직 generator관련
- 이미지 합성과정에서 이미지의 전체적인 스타일과 세세한부분까지 조정이 가능
- CelebA-HQ보다 더 고화질이면서도 훨씬 다양한 종류의 사람 얼굴을 포함하고 있는 Flickr-Faces_HQ(FFHQ)데이터셋을 공개.\
![image](https://user-images.githubusercontent.com/70633080/113823385-8e797300-97b9-11eb-8fe7-d330a57a93e1.png)
- (a) : 기존 generator, (b) : style based generator
- (b)의 경우 z가 바로 convolution에 들어가는 것이 아닌 mapping network를 통과한다.
- 이후 변형된 w를 이미 학습된 텐서에 style을 입히는 방법을 사용
  - 이미학습된 텐서란 학습데이터들의 style이 하나도 가미되지않은 평균얼굴을 표현하는 텐서
 
### 3. Mapping Network
- 목표 : input vector를 각기 다른 시각적 특징을 다른요소로 컨트롤할 수 있는 중간벡터로 인코딩하는것.
  - 학습데이터의 확률밀도를 따라야 하기때문에 인풋벡터로 시각적특징을 컨트롤하는 것은 어려운 일이다.
- 기존 Gan의 generator의 문제점은 **특징들이 서로 얽혀있어 벡터를 조절하면 얽힌 여러특징이 동시에 변하게 된다는 것**이다.
- 이러한 문제를 **entanglement** 그리고 이로인해 인풋의 특징을 제대로 매핑하지 못하는 문제를 **feature entanglement** 라고 부른다.
- 이 entanglement문제를 해결하는 방법이 Mapping Network이다.
- Mapping network는 8개의 fully connected layer로 구성되어있으며 output w은 input layer(512 * 1)과 같은 사이즈이다.\
![image](https://user-images.githubusercontent.com/70633080/113819453-4b68d100-97b4-11eb-9d47-9e2ef2b9afd4.png)
- 이제 여기에 Adaptive Instance Normalization(AdaIN)을 적용하기 위한 추가적인 연산 (아래그림에서의 A)과 noise를 더해주기위한 추가적인 연산(아래그림에서의 B)를 거치면 generator가 완성된다.

### 4.Style Modules(AdaIN)
- AdaIn? : 2017년에 제안도딘 스타일변환용의 정규화 방법이다. 
- 수식은 아래와 같다. 논문에서는 콘텐츠입력 x와 스타일입력 y를 평균과 분산을 이용해 정규화 한다.\
![image](https://user-images.githubusercontent.com/70633080/113818062-368b3e00-97b2-11eb-9d06-c31ea5f928fc.png)
- Instance Normalization등의 정규화방법과 달리 스타일과 콘텐츠 이미지의 총합략만 정규화 하고 학습파라미터를 사용하지 않는다.
- 따라서 훈련데이터에 없던 스타일이라도 스타일변환이 가능하다.
- Style GAN 중 Adain은 아래의 수식을 사용한다.
- 정규화된 콘텐츠 정보에 스타일을 사용한 선형변환을 적용하는 개념은 변화하지않지만 스타일의 표준편차와 평균대신 스타일벡터 W에 선형변환을 더한 y_s, y_b를 사용한다.\
![image](https://user-images.githubusercontent.com/70633080/113818270-836f1480-97b2-11eb-8e1a-0ee0301d8194.png)

- 전체적인 과정은 다음과 같다.
#### 4-1.아핀변환 후 Adain과정
- Mapping network에서 생성된 인코딩된 정보 w로 해당텐서에 스타일을 입힌다.(transfer)\
![image](https://user-images.githubusercontent.com/70633080/113827842-b3241980-97be-11eb-9a1c-c67ff6ccf8f6.png)
- 위 그림에서의 Synthesis network는 4 * 4 * 512 텐서로 시작해 1024 * 1024 * 3 으로 끝나는 8개의 layer로 구성되어 있다.
- 해당 layer마다 upsampling과 convolution operation이 끝난 뒷부분에 Adain을 적용한다.\
![image](https://user-images.githubusercontent.com/70633080/113825536-12ccf580-97bc-11eb-8b82-6f0400f85fe9.png)
- 다만, w는 512개로 Adain을 적용하기엔 채널수와 사이즈가 다르므로 **아핀변환**을 적용한다.
  - 아핀변환(A)를 통해 각 채널의 scale과 bias로 변환된다.
  - scale과 bias는 각 채널의 conv output을 shift시켜 convolution에서 각 필터의 중요성을 정의한다.
- 2n개의 아핀변환후의 output을 n개의 scale , n개의 bias에 사용하여 기존채널에 스타일을 입힌다.
- Convolutiojn output각 채널들을 먼저 정규화 한후, 앞서구한 스타일함수 y(Adain 식)를 각 채널별로 적용한다.
- 이렇게 되면 해당 채널들에 원하는 스타일을 입힐 수 있다.
#### 4-2.Noise추가 (Stochastic variation)
- 다음으로 noise를 추가하는 stochastic variation이다.
- 이는 이미지의 세세한 부분을 바꾸기위해 noise를 더하는 방법이다.
- 적용은 Adain과 동일한 방식이며 랜덤한 가우시안노이즈를 각 채널별로 집어넣는 방식을 사용했다.
