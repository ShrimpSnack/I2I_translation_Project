# 1. starGAN
starGAN의 개념과 특징, 구조에 대해 살펴본다.
- [논문참고]<https://arxiv.org/abs/1812.04948>
## 1. 등장배경
- 기존의 image-to-image translation 연구( Pix2Pix, CycleGAN, DiscoGAN 등)는 3개이상의 domain에서 안정적이지 않다.
- domain수에 따라 Generator의 수가 증가하게 되는 단점이 있다.
  - domain수가 k일때 generator는 k(k-1)개
## 2. 특징
1. Progressive Growing을 이용한 고해상도 이미지를 생성
2. Adain을 이용해 각 층에 이미지의 style을 캡처

- 단일 모델을 사용하여 여러가지의 domain에 대해 img-to-img translation이 가능하다.
  - 한 이미지에서 나이, 성별 등 여러 attribute를 한번에 바꾸기 가능
  - attribute : 이미지에 내재된 의미있는 특징 ex)성별, 나이, 머리색
  - attribute value : attribute의 특정한 값 ex) 10/29, female / male, yellow/black
  - domain : 동일한 attribute value를 가지는 image set이다. ex) gender가 male인 image들이 하나의 domain이다.
- mask ventor를 이용해 다른 domain을 가진 dataset들을 동시에 학습시킬 수 있다.
  - A DB: 나이, 성별, 머리색 /  B DB : 표정, 옷 >> A DB에서 B DB의 특징을 이용해 변환가능\
![image](https://user-images.githubusercontent.com/70633080/109119331-e15a1680-7787-11eb-9cf5-53c2dd6c4101.png) 

## 3. Overview of StarGAN 
![image](https://user-images.githubusercontent.com/70633080/109119763-765d0f80-7788-11eb-8488-4bef923d50d9.png)
- (a) D : x ->  {Dsrc(x), Dcls(x)} D는 real과 fake를 구분함과 동시에 real image일때 해당 domain으로 분류해내는 것을 학습한다. 
  - 즉, D는 source와 domain labels에 대한 확률분포를 만든다.
- (b) G의 input으로 {input img, target domain}이 들어간다.
  - 여기서 target domain은 label(binary or one-hot vector)형태로 들어간다. 
  - output으로 fake이미지를 생성한다.
- (c) G는 original domain label(원래 가지고있던 image의 one-hot vector)를 가지고 fake image를 다시 origin image로 reconstruction하려한다. 
  - 따라서 output은 reconstructed image이다. 
- (d) D를 속이는 과정. G는 real과 구분 불가능하고 D에 의해 target domain으로 분류될 수 있는 이미지를 생성한다.

- **한개의 Generator를 다른 용도로 2번 사용된다.**

## 4. Model structure
- 모델의 구조를 이해하기 위해서는 StyleGAN의 구조이해가 필요하다
- StyleGAN에서 사용된 기법 및 구조는 다음과 같이 있다.
  - 1. Progressive Growing
  - 2. Style-based generator
  - 3. Mapping network
  - 4. AdaIn
### 1. Progressive Growing
- progressive growing? : GAN에서 제안된 고해상도 이미지를 생성하는 방법
- 저해상도 이미지의 생성으로부터 시작해 점차 고해상도용의 Generator, Discriminator를 추가함
- 아래그림은 PG-GAN의 모식도이다.\
![image](https://user-images.githubusercontent.com/70633080/113817847-ef9d4880-97b1-11eb-8863-a43ca841c729.png)
- 4 * 4 의 이미지를 생성한 것으로 부터 시작해서 조금씩 해상도를 높여 최종적으로 1024 * 1024의 고해상도의 이미지를 생성해낸다.
- 해상도를 높이는 네트워크를 추가하여도 저해상도 이미지를 생성하는 G와 D는 파라미터가 고정되지않고 학습을 계속하는 것이 특징이다.
- 레이어(화질)이 낮을수록 내포된 특징이 다르다.
  - 1.Coarse(굵직한 특징) : 8해상도까지 (4 * 4 ~ 8 * 8) -> 포즈, 헤어스타일, 얼굴형에 영향
  - 2. Middle(중간 특징) : 16부터 32해상도 까지 -> 자세한 얼굴특징, 헤어스타일, 눈뜨고감음에 영향
  - 3. Fine(자세한 특징) : 64부터 1024까지 -> 눈,머리,피부등의 색조합과 미세한 특징에 영향 
### 2. Style-based generator
- 이는 2018년 12월의 Nvidia에서 올린 논문의 내용으로 GAN의 한계점을 극복할 수 있는 방향을 제시하였다.
- 어떤 GAN구조에도 바로적용할 수 있는 generator를 제안.
  - discriminator를 수정하거나 손실함수를 바꾸는 등 어떤 변경점도 없는 오직 generator관련
- 이미지 합성과정에서 이미지의 전체적인 스타일과 세세한부분까지 조정이 가능
- CelebA-HQ보다 더 고화질이면서도 훨씬 다양한 종류의 사람 얼굴을 포함하고 있는 Flickr-Faces_HQ(FFHQ)데이터셋을 공개.\
![image](https://user-images.githubusercontent.com/70633080/113823385-8e797300-97b9-11eb-8fe7-d330a57a93e1.png)
- (a) : 기존 generator, (b) : style based generator
- (b)의 경우 z가 바로 convolution에 들어가는 것이 아닌 mapping network를 통과한다.
- 이후 변형된 w를 이미 학습된 텐서에 style을 입히는 방법을 사용
  - 이미학습된 텐서란 학습데이터들의 style이 하나도 가미되지않은 평균얼굴을 표현하는 텐서
 
### 3. Mapping Network
- 목표 : input vector를 각기 다른 시각적 특징을 다른요소로 컨트롤할 수 있는 중간벡터로 인코딩하는것.
  - 학습데이터의 확률밀도를 따라야 하기때문에 인풋벡터로 시각적특징을 컨트롤하는 것은 어려운 일이다.
- 기존 Gan의 generator의 문제점은 **특징들이 서로 얽혀있어 벡터를 조절하면 얽힌 여러특징이 동시에 변하게 된다는 것**이다.
- 이러한 문제를 **entanglement** 그리고 이로인해 인풋의 특징을 제대로 매핑하지 못하는 문제를 **feature entanglement** 라고 부른다.
- 이 entanglement문제를 해결하는 방법이 Mapping Network이다.
- Mapping network는 8개의 fully connected layer로 구성되어있으며 output w은 input layer(512 * 1)과 같은 사이즈이다.\
![image](https://user-images.githubusercontent.com/70633080/113819453-4b68d100-97b4-11eb-9d47-9e2ef2b9afd4.png)
- 이제 여기에 Adaptive Instance Normalization(AdaIN)을 적용하기 위한 추가적인 연산 (아래그림에서의 A)과 noise를 더해주기위한 추가적인 연산(아래그림에서의 B)를 거치면 generator가 완성된다.

### 4.Style Modules(AdaIN)
- AdaIn? : 2017년에 제안도딘 스타일변환용의 정규화 방법이다. 
- 수식은 아래와 같다. 논문에서는 콘텐츠입력 x와 스타일입력 y를 평균과 분산을 이용해 정규화 한다.\
![image](https://user-images.githubusercontent.com/70633080/113818062-368b3e00-97b2-11eb-9d06-c31ea5f928fc.png)
- Instance Normalization등의 정규화방법과 달리 스타일과 콘텐츠 이미지의 총합략만 정규화 하고 학습파라미터를 사용하지 않는다.
- 따라서 훈련데이터에 없던 스타일이라도 스타일변환이 가능하다.
- Style GAN 중 Adain은 아래의 수식을 사용한다.
- 정규화된 콘텐츠 정보에 스타일을 사용한 선형변환을 적용하는 개념은 변화하지않지만 스타일의 표준편차와 평균대신 스타일벡터 W에 선형변환을 더한 y_s, y_b를 사용한다.\
![image](https://user-images.githubusercontent.com/70633080/113818270-836f1480-97b2-11eb-8e1a-0ee0301d8194.png)

- 전체적인 과정은 다음과 같다.
#### 4-1.아핀변환 후 Adain과정
- Mapping network에서 생성된 인코딩된 정보 w로 해당텐서에 스타일을 입힌다.(transfer)\
![image](https://user-images.githubusercontent.com/70633080/113827842-b3241980-97be-11eb-9a1c-c67ff6ccf8f6.png)
- 위 그림에서의 Synthesis network는 4 * 4 * 512 텐서로 시작해 1024 * 1024 * 3 으로 끝나는 8개의 layer로 구성되어 있다.
- 해당 layer마다 upsampling과 convolution operation이 끝난 뒷부분에 Adain을 적용한다.\
![image](https://user-images.githubusercontent.com/70633080/113825536-12ccf580-97bc-11eb-8b82-6f0400f85fe9.png)
- 다만, w는 512개로 Adain을 적용하기엔 채널수와 사이즈가 다르므로 **아핀변환**을 적용한다.
  - 아핀변환(A)를 통해 각 채널의 scale과 bias로 변환된다.
  - scale과 bias는 각 채널의 conv output을 shift시켜 convolution에서 각 필터의 중요성을 정의한다.
- 2n개의 아핀변환후의 output을 n개의 scale , n개의 bias에 사용하여 기존채널에 스타일을 입힌다.
- Convolutiojn output각 채널들을 먼저 정규화 한후, 앞서구한 스타일함수 y(Adain 식)를 각 채널별로 적용한다.
- 이렇게 되면 해당 채널들에 원하는 스타일을 입힐 수 있다.
#### 4-2.Noise추가 (Stochastic variation)
- 다음으로 noise를 추가하는 stochastic variation이다.
- 이는 이미지의 세세한 부분을 바꾸기위해 noise를 더하는 방법이다.
- 적용은 Adain과 동일한 방식이며 랜덤한 가우시안노이즈를 각 채널별로 집어넣는 방식을 사용했다.

## 5. Loss
- 전체 Loss
![image](https://user-images.githubusercontent.com/70633080/109122147-82969c00-778b-11eb-84c7-6a4e00668b81.png)

### 1. Adversarial Loss
![image](https://user-images.githubusercontent.com/70633080/109122231-a0640100-778b-11eb-9678-848403c2e89f.png)
- G(x,c) : x와 target domain label을 가지고 G(x,c)라는 이미지를 생성한다. 
- D는 real과 fake를 구분하려는 loss
- D가 real로 분류할 경우 : 1에 가까운 값으로 출력된다. 
- D가 fake로 분류할 경우 : 0에 가까운 값으로 출력된다.
- real img인 x에 대해선 Dsrc(x)는 1을 가지도록, Dsrc(G(x,c))에서는 G가 real인거처럼 학습하므로 1을 가지도록 학습
- 따라서 LOSS가 최소가 됨 (-log는 1에가까우면 0으로, 0에 가까우면 무한대로 발산)

### 2. Domain Classification Loss
- input img x와 target domain label c가 주어졌을 때, x가 output img y로 변환되어 이것이 target domain c로 분류되는 것이 목적이다. 
- 따라서, D와 G를 optimize할때 domain classification loss를 부과한다.

#### 2-1. domain classification loss of real images used to optimize D
![image](https://user-images.githubusercontent.com/70633080/109123102-cccc4d00-778c-11eb-9134-d5f048c524af.png)
- real image가 들어오면 real image에 대한 original domain값으로 분류되게 하는 loss이다.
- D를 위해 사용되는 loss ( 얼마나 잘 분류했는가)

#### 2-2. domain classification loss of fake images used to optimize G
![image](https://user-images.githubusercontent.com/70633080/109123339-10bf5200-778d-11eb-9285-a1f28e234344.png)
- 생성된 fake img(target domain으로 변환된 이미지)가 target domain으로 분류될수 있도록 LOSS를 최소화한다.
- G를 위해 사용되는 loss ( 얼마나 잘 속였는가)

### 3. Reconstruction Loss
![image](https://user-images.githubusercontent.com/70633080/109124255-0c476900-778e-11eb-92f2-d7546d9aa5f5.png)
- G가 생성한 fake image와 바뀐 target domain(origin label)을 다시 G의 input으로 넣는다.
- fake image를 origin image의 형태로 다시 복원한 image가 출력됨.
- 따라서 **target domain부분은 변화시키되 input image의 형태를 유지하게끔 복원하기 위해 cycle consistence loss**를 이용한다.

### 4. Total loss
![image](https://user-images.githubusercontent.com/70633080/109122231-a0640100-778b-11eb-9678-848403c2e89f.png)
- λcls, λrec : 하이퍼파라미터임. domain분류와 reconstruction loss들의 상대적인 중요도를 컨트롤함.
  - 논문에서 λcls = 1로, λrec=10으로 설정
- D는 adversarial loss를 maximize하길 원하기 때문에 마이너스가 붙은 것이다.
- G는 adversarial loss를 minimize하길 원하므로 마이너스가 붙지 않은 것이다.  

## 6. Mask vector
- 논문에서 사용된 dataset은 ficial attribute만 가지고 있는 CelebA와 facial expression만 가지고 있는 RaFD를 사용한다.
- 이 둘은 서로 다른 도메인이기 때문에 저자는 mask vector , m 이라는 개념을 도입했다.
- m은 one-hot vector로 나타내지고 두 dataset을 합칠때는 concate하면 된다.
``` ~c = [c1,c2,...,cn,m]```
- ci : i번째 dataset의 label들의 vector , ci는 binary attribute를 가진 binary vector 또는 categorical attribute를 가진 one-hot vector이다.
- CelebA와 RaFD를 교차시킴으로써 D는 두 dataset에서 차이를 구분짓는 모든 feature들을 학습하게 된다.
- G는 모든 label을 컨트롤하는 것을 학습하게 된다.

## 7. 전체적인 구조
![image](https://user-images.githubusercontent.com/70633080/109127529-e328d780-7791-11eb-9830-0901b0b10ca4.png)

## 8. Result
![image](https://user-images.githubusercontent.com/70633080/109127607-f5a31100-7791-11eb-8429-099c88090ba2.png)\
![image](https://user-images.githubusercontent.com/70633080/109127658-0489c380-7792-11eb-9ad6-9a42c09e4a2e.png)
- Celeb A와 RaFD를 128 * 128로 동일하게 맞춰준 후 모델에 입력한다.
- Celeb A에서는 40개의 attribute 중 7개만 뽑아 사용했고 RaFD는 작은 dataset이기 때문에 모두 사용한다.
- 여러개의 dataset을 이용한 StarGAN joint는 dataset을 모두 사용해 좋은 성능을 보인다.
- multiple domains + multiple datasets 학습가능ㅇㅇㄴㄹ

# 2. starGAN v2
## 1. starGAN v1 -> v2
- starGAN
  - starGAN v1은 각 domain에 대한 결정을 한번에 하나씩 직접해야한다.
  - 데이터 분포에 대한 다양한 특성을 반영하지 못함
- starGAN v2
  - 어떤 domain의 image한개를 target domain의 여러 다양한 image로 변경할 수 있다.
  - 특정 도메인에 대한 다양한 style들을 표현할 수 있다.
## 2. key point
- Mapping Network :  임의의 가우스 노이즈를 스타일 코드로 변환하는 것을 학습
- Style Encoder : 주어진 소스 이미지에서 스타일 코드를 추출하는 것을 학습

## 3. FrameWork
![image](https://user-images.githubusercontent.com/70633080/109932836-0c0b1880-7d0e-11eb-9916-db3a25623aa6.png)
- X를 이미지의 집합 그리고 Y를 가능한 도메인의 집합이라고 가정
- X에 속하는 이미지 x와 Y에 속하는 도메인 y가 주어졌을때, StarGAN v2의 목표는 하나의 generator만으로 이미지 x를 도메인 y의 이미지로 변형하되, 다양한 스타일로 변형할 수 있도록 학습하는 것이다. 

- (a) Generator : G의 역할은 input image가 들어오면 output으로 G(x,s)가 나온다.
  - s는 style vector로 AdalN(Adaptive instance normalization)을 통해 주입된다.
  - s는 도메인 y의 style을 대표하도록 mapping network F나 style encoder E에 의해 생성된다.
  
- (b) Mapping network : random latent vector z와 domain y가 주어졌을때 Mapping network인 F는 style vector s=Fy(z)를 만든다.
  - 즉, domain y를 대표하는 latent vector z를 style vector s로 mapping해준다. 
  - F는 다중출력 MLP로 구성된다.

- (c) Style Encoder : image x와 domain y가 주어지면 E는 image x에서 style information을 추출하는 역할을 한다. s=Ey(x)

- (d) Discriminator : D는 다중출력 Discriminator이다. D의 각 branch는 이미지 x가 real인지 fake인지 이진분류할 수 있도록 학습한다.

## 4. Training objectives
![image](https://user-images.githubusercontent.com/70633080/109937884-bb49ee80-7d12-11eb-877d-9e205221c45a.png)
1. Adversarial objective
    - StarGAN에서 봤던것과 동일. 
    - 특징은 latent vector z와 타깃도메인 ~y를 랜덤하게 샘플링해, target style vector ~s를 input으로 넣었다는 것이다.

2. Style reconstruction
    - style에 맞게 잘 변화시키기 위한 것이다.
    - ~ s = F ~ y(z)
    - fake image를 만드는데 사용한 style code와 만들어진 fake image를 단일인코더 E에 넣어 얻은 style code를 비교하는 것이다.
    - fake image에 우리가 원하는 스타일코드 ~ s가 많이 적용되었을 수록 인코더를 통과한 fake image ~ s랑 비슷해질 것이다.
    - 즉, 얼마나 우리가 원하는 style에 가깝게 fake image가 생성되었는가를 판단해주는 loss라고 할 수 있다.

3. Style diversification
    - 다양한 style을 생성하게 하기위해 추가된 loss이다.
    - 여기서 s1,s2는 각각 다른 latent vector z에서 생성된 style vector이다.
    - 이 loss는 최적화 지점이 없기 때문에 선형적으로 weight를 0으로 줄여가며 학습했다고 한다.

4. Cycle consistency loss
    - 위의 loss들만으로 생성된 이미지가 input image x에 대해 도메인이 해당하지 않는 속성들을 갖고있는지 확신할 수 없다.
    - 따라서 이를 추가하였음
    - s^은  Ey(x)로 input image x에 대해서 추출된 style vector이다.

5. Full objective 
![image](https://user-images.githubusercontent.com/70633080/109941809-dfa7ca00-7d16-11eb-9f1d-4a778143891c.png)
    - 람다는 하이퍼파라미터이다. adversarial loss를 기준으로 각 loss의 중요도를 반영해 정해진다고 한다.
    - style diversification에서 본것 처럼 weight를 줄여가는 식으로 loss가 구성되었음을 알 수 있다.

## 5. Result
![image](https://user-images.githubusercontent.com/70633080/109941965-0bc34b00-7d17-11eb-8e74-9c8c2097a8df.png)\
![image](https://user-images.githubusercontent.com/70633080/109942005-167de000-7d17-11eb-8a49-346d1da1e8fd.png)


# 참고문헌
- <https://velog.io/@tobigs-gm1/Multidomain-ImageTranslation>

